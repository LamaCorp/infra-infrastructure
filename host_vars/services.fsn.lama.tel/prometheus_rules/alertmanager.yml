---

# Alerts from:
# - https://monitoring.mixins.dev/node-exporter/
# - https://awesome-prometheus-alerts.grep.to/rules#prometheus-self-monitoring

prometheus_rules_alertmanager:
  alertmanager:
    groups:
      - name: alertmanager.alerts
        rules:
          - alert: AlertmanagerJobMissing
            expr: absent(up{job = "alertmanager"})
            for: 1m
            labels:
              severity: warning
            annotations:
              summary: Prometheus Alertmanager job missing.
              description: !unsafe 'A Prometheus AlertManager job has disappeared.'

          - alert: AlertmanagerNotificationFailing
            expr: rate(alertmanager_notifications_failed_total{job = "alertmanager"}[1m]) > 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Prometheus AlertManager notifications are failing.
              description: !unsafe 'Alertmanager {{ $labels.instance }} is failing to send notifications.'

          - alert: AlertmanagerFailedReload
            # Without max_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            expr: max_over_time(alertmanager_config_last_reload_successful{job = "alertmanager"}[5m]) == 0
            for: 10m
            labels:
              severity: critical
            annotations:
              summary: Reloading an Alertmanager configuration has failed.
              description: !unsafe 'Configuration has failed to load for {{ $labels.instance }}.'

          - alert: AlertmanagerMembersInconsistent
            # Without max_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            expr: |
                max_over_time(alertmanager_cluster_members{job = "alertmanager"}[5m])
                < on (job) group_left
                  count by (job) (max_over_time(alertmanager_cluster_members{job = "alertmanager"}[5m]))
            for: 15m
            labels:
              severity: critical
            annotations:
              summary: A member of an Alertmanager cluster has not found all other cluster members.
              description: !unsafe 'Alertmanager {{ $labels.instance }} has only found {{ $value }} members of the {{ $labels.job }} cluster.'

          - alert: AlertmanagerFailedToSendAlerts
            expr: |
              (
                rate(alertmanager_notifications_failed_total{job = "alertmanager"}[5m])
              /
                rate(alertmanager_notifications_total{job = "alertmanager"}[5m])
              )
              > 0.01
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: An Alertmanager instance failed to send notifications.
              description: !unsafe 'Alertmanager {{ $labels.instance }} failed to send {{ $value | humanizePercentage }} of notifications to {{ $labels.integration }}.'

          - alert: AlertmanagerClusterFailedToSendAlerts
            expr: |
              min by (job, integration) (
                rate(alertmanager_notifications_failed_total{job = "alertmanager", integration =~ ".*"}[5m])
              /
                rate(alertmanager_notifications_total{job = "alertmanager", integration =~ ".*"}[5m])
              )
              > 0.01
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: All Alertmanager instances in a cluster failed to send notifications to a critical integration.
              description: !unsafe 'The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.'

          - alert: AlertmanagerClusterFailedToSendAlerts
            expr: |
              min by (job, integration) (
                rate(alertmanager_notifications_failed_total{job = "alertmanager", integration !~ ".*"}[5m])
              /
                rate(alertmanager_notifications_total{job = "alertmanager", integration !~ ".*"}[5m])
              )
              > 0.01
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: All Alertmanager instances in a cluster failed to send notifications to a non-critical integration.
              description: !unsafe 'The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.'

          - alert: AlertmanagerConfigInconsistent
            expr: |
              count by (job) (
                count_values by (job) ("config_hash", alertmanager_config_hash{job = "alertmanager"})
              )
              != 1
            for: 20m
            labels:
              severity: critical
            annotations:
              summary: Alertmanager instances within the same cluster have different configurations.
              description: !unsafe 'Alertmanager instances within the {{$labels.job}} cluster have different configurations.'

          - alert: AlertmanagerClusterDown
            expr: |
              (
                count by (job) (
                  avg_over_time(up{job = "alertmanager"}[5m]) < 0.5
                )
              /
                count by (job) (
                  up{job = "alertmanager"}
                )
              )
              >= 0.5
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: Half or more of the Alertmanager instances within the same cluster are down.
              description: !unsafe '{{ $value | humanizePercentage }} of Alertmanager instances within the {{ $labels.job }} cluster have been up for less than half of the last 5m.'

          - alert: AlertmanagerClusterCrashlooping
            expr: |
              (
                count by (job) (
                  changes(process_start_time_seconds{job = "alertmanager"}[10m]) > 4
                )
              /
                count by (job) (
                  up{job = "alertmanager"}
                )
              )
              >= 0.5
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: Half or more of the Alertmanager instances within the same cluster are crashlooping.
              description: !unsafe '{{ $value | humanizePercentage }} of Alertmanager instances within the {{ $labels.job }} cluster have restarted at least 5 times in the last 10m.'
